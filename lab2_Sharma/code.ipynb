{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Reinforcement Learning for Self Driving Car**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The code below is taken from Nicholas Renotte's tutorial on how to guide a racing car with reinforcement learning. [Tutorial](https://youtu.be/Mut_u40Sqz4?t=7595), [code on github](https://github.com/nicknochnack/ReinforcementLearningCourse/blob/main/Project%202%20-%20Self%20Driving.ipynb).\n",
                "\n",
                "You are encouraged to visit the links above and check out the full code. In this lab, you will practice training a model."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**About the problem**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The goal is to train a reinforcement learning agent to drive a race car around a track.\n",
                "We will use the racing car environment from Gym environments."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Import libraries**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "# Avoid reinstalling packages that are available on edstem\n",
                "if not os.getenv(\"ED_COURSE_ID\"):\n",
                "    !pip install tensorflow stable_baselines3 torch collections gym box2d-py --user\n",
                "\n",
                "# Import gym libraries\n",
                "import gym \n",
                "\n",
                "#Import stable bbaselines libraries\n",
                "from stable_baselines3 import PPO\n",
                "from stable_baselines3.common.vec_env import VecFrameStack\n",
                "from stable_baselines3.common.evaluation import evaluate_policy"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Acquire the environment from Gym"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make the racing car environment from the Gym library\n",
                "environment_name = \"CarRacing-v1\"\n",
                "env = gym.make(environment_name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#TODO: Write code to inspect the action space of the environment\n",
                "print(env.action_space)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "#TODO: Write code to inspect the observation space of the environment\n",
                "print(env.observation_space)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Test the environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Trigger Ed's X display to visualize the environment\n",
                "!xdpyinfo\n",
                "\n",
                "# Test five episodes of taking random Actions\n",
                "# in the environment\n",
                "episodes = 5\n",
                "for episode in range(1, episodes+1):\n",
                "    state = env.reset()\n",
                "    done = False\n",
                "    score = 0 \n",
                "    \n",
                "    while not done:\n",
                "        env.render()\n",
                "        action = env.action_space.sample()\n",
                "        n_state, reward, done, info = env.step(action)\n",
                "        score+=reward\n",
                "        print('Episode:{} Score:{}'.format(episode, score))\n",
                "    \n",
                "env.close()\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Earn Your Wings"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Implement the rest of the reinforcement learning algorithm to train the model using CnnPolicy. Save the training in the log_path defined below, and evaluate the model at the end with render set to False. Add comments in your code to explain each step that you take in your implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "log_path = os.path.join('/tmp/ReinforcementLearning/SelfDriving/Training', 'Logs')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "from stable_baselines3 import PPO\n",
                "from stable_baselines3.common.vec_env import VecFrameStack\n",
                "from stable_baselines3.common.evaluation import evaluate_policy\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = gym.make(environment_name)\n",
                "# Creates the Gym"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = PPO(\"CnnPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
                "# Creates model based on env with cnnpolicy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.learn(total_timesteps=40000)\n",
                "#trains for 40k timesteps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_Driving_Model')\n",
                "model.save(ppo_path)"
            ]
        }
    ]
}
